<!--

Spring 2018

2018-06-04 : Initial version DL

=====================================================================
triggerRate

Most of the time was spent at 150nA, but for the second half of
the run we split the time between the 75um and 750um TPOL convertor.
The trigger rate was 50kHz and 70kHz respectively. We estimate 3/4
of the time spent at 50kHz and 1/4 at 70kHz giving a 56.25kHz average.

=====================================================================
runningTimeOnFloor

Alexandre's slides from Feb. 22 indicated we were partway into a
10 week run that would end on Mar. 21st. We actually ran until
the morning of May 6. According to Curtis' spreadsheet, we had
88 days = 12.6 weeks of running total. That likely takes into
account all beam studies/rf recovery, and the actual longer down
times during the run. (There was some extended downtime from
Mar. 6-24).

https://halldweb.jlab.org/DocDB/0035/003526/002/Fall18_Spring18_summary.pdf
https://docs.google.com/spreadsheets/d/1NffTc4-S5PbTMSuH_pp7ZYsGRH4JC_WSVo770_iJYt0/edit#gid=1635471172

We use the 88 days number.

=====================================================================
runningEfficiency

This number (39%) was tuned to make the total production close to, but
less than the 1910TB reported by the scicomp web page. Nominally, the
acclerator runs at around 50%, but our overall running efficiency is
supposed to represent production running only. Special runs and losses
due to DAQ etc. count against this number.

=====================================================================
eventsize

Multiple files from run 42323 have 1.7M events in them (some have more).
Use an avg. event size of 19.1GB/1.7Mevents = 11.2kB/event. Some files
have fewer event so we increase this to 11.5kB/event. This matches the
number the hdevio_scan histogram reports for file 001.

=====================================================================
eventsPerRun

Number of events (in millions) in a production run. As we alternated
between 75um and 750um convertor, we also alternated between 200M
and 400M events per run. We use the average of 300M.

=====================================================================
RESTfraction

The number used for RunPeriod-2017-01 was based on files that were 14.6%
the size of the 20GB raw data files. Here we round up to 15%.

n.b. This is the ratio of the REST file size to the uncompressed
raw data size.

=====================================================================
reconstructionRate

Directly measured Spring 2018 run 42323 on gluon112 gives about 7.2Hz
per full core based on an Amdahl fit. If I read off the 24 thread
rate from the curve of the plot, it is about 160Hz which would be
about 6.67Hz/core. I'm not sure I understand that discrepancy.
For 2017 recon jobs, it looked closer to 5Hz/core. This would have
included hyperthreads, but also writing of REST files.

Another complication is that the CPU hours does not look like it matches
perfectly with the wall hours. Specifically. looking at plots on the
following page shows the peak in "time spent in active" to be at 4 hrs
while the peak in the "cpu time" plot is at 90. 90/24=3.75hrs.
https://halldweb.jlab.org/data_monitoring/recon/summary_swif_output_recon_2017-01_ver02_batch02.html

For the purposes of this estimate, we'll assume 5.5Hz.

=====================================================================
reconPasses

Expect equivalent of 2.5 full reconstruction passes.

=====================================================================
goodRunFraction

This is taken from the RunPeriod-2017-01 number of 85%

=====================================================================
analysisRate

We use the rate for RunPeriod-2017-01 of 75Hz

=====================================================================
analysisPasses

Use Alex A.'s advice to have 2 analysis passes per recon pass:

  2 x 2.5 = 5.0

=====================================================================
cores

Based on conversations with Chip, assume 10000 cores for now. The goal
is to do most of the recon pass at NERSC.

=====================================================================
incomingData

proportional to number of runs

Number of files per run analyzed for the "incoming data" jobs. This
is always 5.

=====================================================================
calibRate

proportinal to time on floor

This value represents the number of Mhr of CPU used per week of running
to calibrate the detector. The value of 0.530 is taken from the
RunPeriod-2017-01 number.

=====================================================================
offlineMonitoring

proportional to number of runs

The value of 0.00800 is taken from the RunPeriod-2017-01 number.

=====================================================================
miscUserStudies

proportional to time to process al files of single run

This value is used to capture the CPU usage by all of the various users
that is attributed to the gluex project. Some of this should probably
go under calibRate, but it is very hard to categorize which parts of
this should go there.

It is assumed here that these are jobs that run over all files from a
small number of runs in order to do special studies. The amount of CPU
required is therefore proportional to the time it takes to process a
single production run.

The value of 810 is taken from the RunPeriod-2017-01 number.

=====================================================================
simulationRate

This is based on a very rough value Thomas B. gave of 40ms/event for
bggen events with real data background mixed in. Note that adding the
background this way significantly reduced the compute time required
from previous models.

=====================================================================
simulationpasses

Number of times we will need to repeat simulation. This value of
2 is an old estimate.

=====================================================================
simulatedPerRawEvent

Number of simulated events needed for each raw data event (production
runs only) This is assumed to be 2 simulated events for each signal
event in the raw data stream. We estimate about 20% of the raw data
is reconstructable (see "GlueX at High Intensity" talk slide 10
here:  https://halldweb.jlab.org/wiki/index.php/GlueX-II_and_DIRC_ERR )


-->
<compMod>
<parameter name="triggerRate" value="56.25e3" units="Hz"/>
<parameter name="runningTimeOnFloor" value="88.0" units="days"/>
<parameter name="runningEfficiency" value="0.39"/>
<parameter name="eventsize" value="11.5" units="kB"/>
<parameter name="eventsPerRun" value="300" units="Mevent"/>
<parameter name="compressionFactor" value="1.0"/>
<parameter name="RESTfraction" value="0.15"/>

<parameter name="reconstructionRate" value="5.5" units="Hz"/>
<parameter name="reconPasses" value="2.5"/>
<parameter name="goodRunFraction" value="0.85"/>
<parameter name="analysisRate" value="75.0" units="Hz"/>
<parameter name="analysisPasses" value="5.0"/>
<parameter name="cores" value="4500"/>
<parameter name="incomingData" value="5" units="files"/>
<parameter name="calibRate" value="0.530" units="Mhr/week"/>
<parameter name="offlineMonitoring" value="0.00800" units="Mhr/run"/>
<parameter name="miscUserStudies" value="810"/>

<parameter name="simulationRate" value="25" units="Hz"/>
<parameter name="simulationpasses" value="2"/>
<parameter name="simulatedPerRawEvent" value="0.4"/>


</compMod>
